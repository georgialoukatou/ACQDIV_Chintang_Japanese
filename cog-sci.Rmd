---
title: "Supplementary materials to wordseg-xling"
author: "alejandrina cristia"
date: "4/27/2018"
output: html_document
---

```{r setup, include=FALSE}

RECALC=FALSE #turn TRUE if you need to re-run the analyses from scratch (i.e. results or stats have changed)

knitr::opts_chunk$set(echo = TRUE)
args<-commandArgs(trailingOnly=TRUE)
#args<-commandArgs(trailingOnly=TRUE)
#resfile=args[1]
#statfile=args[2]
#folder=args[3]
#outfolder=args[4]
resfile<-"acqdiv/results.txt"
statfile="acqdiv/stats.txt"
folder="/scratch1/users/acristia/acqdiv/" #name of the root where files where stored
outfolder="~/Documents/acqdiv/" #location you want all figures and temp data to be stored in

```

## Data pre-processing



```{r intro, eval=RECALC}

#COMPILE RESULTS DATA
data<-read.csv(resfile)
for(thiscol in c("boundary_NE_precision" ,"boundary_NE_recall" ,"boundary_NE_fscore")) data[,thiscol]=as.numeric(as.character(data[,thiscol]))
summary(data)
data$fscores=data$token_fscore

#clean up and extract info from files named like /scratch1/users/acristia/acqdiv//Chintang/morphemes/full-tags.txt_eval.ag.txt
# and like Japanese/words/full_part9/tags.txt_eval.ftp_rel.txt
data$folder=gsub("//","/",data$folder)
data$folder=gsub(folder,"",data$folder)
data$language=gsub("/.*","",data$folder)
data$level=gsub("/.*","",gsub(".*[ge]/","",data$folder))
data$subalgorithm=gsub(".txt","",gsub(".*eval.","",data$folder))
data$subalgorithm=gsub("ag","AG",data$subalgorithm)
data$subalgorithm=gsub("linesyll","",data$subalgorithm)
data$subalgorithm=gsub("btp_","BTP",data$subalgorithm)
data$subalgorithm=gsub("ftp_","FTP",data$subalgorithm)
data$subalgorithm=gsub("rel","r",data$subalgorithm)
data$subalgorithm=gsub("abs","a",data$subalgorithm)
data$subalgorithm=gsub("dibs","DiBS",data$subalgorithm)

data$file=gsub("tags.txt.*","",data$folder)
  
# divergence in notation between full and part
data$corpus=NA
data$corpus[grep("_part",data$folder,invert=T)]=gsub("-tags","",gsub("\\..*","",gsub(".*/","",data$folder[grep("_part",data$folder,invert=T)])))
data$subcorpus=NA
data$subcorpus[grep("_part",data$folder)]=gsub(".*/","",gsub("/tags.*","",data$folder[grep("_part",data$folder)]))

summary(data)

#COMPILE STATS DATA
stat<-read.csv(statfile)
stat$folder=gsub("//","/",stat$folder)
stat$folder=gsub(folder,"",stat$folder)
stat$file=gsub("stats.txt.*","",stat$folder)

#combine, derive some stats, and save
merge(data,stat,by="file")->data

data$syllablesPerUtt=data$syllables_tokens/data$corpus_nutts
data$phonesPerWord=data$phones_tokens/data$words_tokens
data$words_hapax_ratio=data$words_hapaxes/data$words_types
data$wtokens_wtypes_ratio=data$words_tokens/data$words_types

write.table(data[grep("full",data$file),],paste0(outfolder,"/res-stat.txt"),row.names=F,quote=T,sep=",")
write.table(data[grep("noforeign",data$file),],paste0(outfolder,"/res-stat-noforeign.txt"),row.names=F,quote=T,sep=",")

```

## Corpus statistics section
```{r corstats, echo=FALSE}
stat<-read.csv(statfile)
stat$folder=gsub("//","/",stat$folder)
stat$folder=gsub(folder,"",stat$folder)
stat$file=gsub("stats.txt.*","",stat$folder)
stat$part=F
stat$part[grep("part",stat$file)]<-T
stat$full=F
stat$full[grep("full",stat$file)]<-T
stat$language=gsub("/.*","",stat$folder)
stat$level=gsub("/.*","",gsub(".*[ge]/","",stat$folder))
ch_w=stat[!stat$part & stat$full & stat$language=="Chintang" & stat$level=="words",]
ch_m=stat[!stat$part & stat$full & stat$language=="Chintang" & stat$level=="morphemes",]
j_w=stat[!stat$part & stat$full & stat$language=="Japanese" & stat$level=="words",]
j_m=stat[!stat$part & stat$full & stat$language=="Japanese" & stat$level=="morphemes",]
```

**The code above allowed us to fill in this paragraph:**

All child-directed and child-overheard speech had been carefully transcribed in a transparent orthography (as well as the child's own speech, which was not analyzed here). After processing, the Chintang corpus contained  `r ch_w$words_tokens` utterances, with an average of `r round(ch_w$words_tokens/ch_w$corpus_nutts,2)` words, `r round(ch_w$syllables_tokens/ch_w$corpus_nutts,2)` syllables, and `r round(ch_w$phones_tokens/ch_w$corpus_nutts,2)` phones per utterance; and the Japanese one `r j_w$words_tokens` utterances, with an average of `r round(j_w$words_tokens/j_w$corpus_nutts,2)` words, `r round(j_w$syllables_tokens/j_w$corpus_nutts,2)` syllables, and `r round(j_w$phones_tokens/j_w$corpus_nutts,2)` phones per utterance. 

## General results section

```{r res-par1, echo=FALSE}
read.csv(paste0(outfolder,"/res-stat.txt"))->data
data$subalgorithm<-factor(data$subalgorithm,c("AG", "BTPa", "FTPa", "BTPr", "FTPr", "DiBS","base1","base0"))
data$level<-factor(data$level,c("words", "morphemes"))
subset(data,!is.na(subcorpus))->parts
subset(data,!is.na(corpus))->wholes 

mainLM <- lm(fscores ~ language * level * subalgorithm + (1 / file), parts)
 f <- summary(mainLM)$fstatistic
 p <- pf(f[1],f[2],f[3],lower.tail=F)
 coefs <- summary(mainLM)$coefficients
```

**The code above allowed us to fill in this paragraph:**

Throughout the analyses, we focused on token F-scores separately for each combination of language, level, and type of algorithm. A regression predicting F-scores from language, level, algorithm and their interactions accounted for most of the variance in the data, $R^2=$.`r round(summary(mainLM)$adj.r.squared,2)` ($F(`r summary(mainLM)$df[1]-1`, `r summary(mainLM)$df[2]`) $=$ `r round(summary(mainLM)$fstatistic[1],3)`, $p < $ `r round(p,3)`). As predicted, the coefficient estimating the language effect between Chintang and Japanese is positive (`r round(coefs["languageJapanese","Estimate"],2)`), suggesting higher scores for the latter. Interestingly, this effect is smaller than that of level (`r round(coefs["levelmorphemes","Estimate"],2)`, indicating evaluation on words lead to lower scores than that on morphemes). The presence of all 2- and 3-way interactions, however, discourages a simple reading of these main effects.  

```{r fig:res, echo=FALSE}
mycols<-palette(c("black", "red", "blue", "orange", "gray", "violet", "brown","green1"))
newdata <- parts

newdata$xval= ifelse(newdata$level=="words",0,2) + ifelse(newdata$language=="Chintang",1,2) + (as.numeric(newdata$subalgorithm)/9 - mean(as.numeric(newdata$subalgorithm)/9))
newdata$col=mycols[as.numeric(newdata$subalgorithm)]

wholes$xval=  ifelse(wholes$level=="words",0,2) + ifelse(wholes$language=="Chintang",1,2) + (as.numeric(wholes$subalgorithm)/9 - mean(as.numeric(wholes$subalgorithm)/9)) -.03
wholes$col=mycols[as.numeric(wholes$subalgorithm)]

pdf(paste0(outfolder,"res.pdf"),width=8,height=5)
par(mar=c(3,3,0.5,0.5), xpd=TRUE)
plot(newdata$token_fscore~newdata$xval,col=newdata$col, pch="x",xaxt='n',xlab="", ylab='', yaxt='n', ylim=c(0,0.8), cex.lab = 1.3, cex.axis = 1, main="", cex.main=1.3)
axis(1,at=1:4,labels=c("Chintang","Jap.","Chintang","Jap."), col = NA, cex.axis = 1.3,line=-.5)
axis(1,at=c(1.5,3.5),labels=c("Words","Morphemes"), col = NA, cex.axis = 1.3, line=.5)
axis(4,line=-.2,col=NA,col.ticks=1,labels=F)
axis(2,line=-.2,col=NA,col.ticks=1,labels=F)
axis(2,line=-.8,col=NA,cex=1.3)
mtext('Token F-score',2,line=1.2, cex=1.3)

points(wholes$token_fscore~wholes$xval,col=wholes$col, pch=20,cex=1.5)

legend('topleft', inset=c(0,0), legend=levels(newdata$subalgorithm), pch=rep("x",length(levels(newdata$subalgorithm))), col=mycols, cex=1, pt.cex=1.5,ncol=4,x.intersp=0.5)

dev.off()
```

Token F-scores across language and representational level. Models are marked by color. Datapoints spread vertically are the results for the ten subsets of each corpus.

```{r tab:meanscores, echo=FALSE}
aggregate(parts$token_fscore,by=list(parts$subalgorithm,parts$language,parts$level),mean)->tab
mytab=as.character(tab$Group.1[tab$Group.2=="Chintang" & tab$Group.3=="words"])
for(thislang in levels(tab$Group.2)) for(thislev in levels(tab$Group.3)) {mytab=cbind(mytab,tab$x[tab$Group.2==thislang & tab$Group.3==thislev]) ; colnames(mytab)[dim(mytab)[2]]<-paste(thislang,thislev)}
colnames(mytab)[1]<-"Algorithm"
mytab

#mean fscores
# agcw<-mean(data[which(data$language=="Chintang" & data$level=="word" & data$subalgorithm=="AGu"),]$token_fscore)
# agcm<-mean(data[which(data$language=="Chintang" & data$level=="morphemes" & data$subalgorithm=="AGu"),]$token_fscore)
# agjw<-mean(data[which(data$language=="Japanese" & data$level=="word" & data$subalgorithm=="AGu"),]$token_fscore)
# agjm<-mean(data[which(data$language=="Japanese" & data$level=="morphemes" & data$subalgorithm=="AGu"),]$token_fscore)
# 
# btpacw<-mean(data[which(data$language=="Chintang" & data$level=="word" & data$subalgorithm=="BTP_abs"),]$token_fscore)
# btpacm<-mean(data[which(data$language=="Chintang" & data$level=="morphemes" & data$subalgorithm=="BTP_abs"),]$token_fscore)
# btpajw<-mean(data[which(data$language=="Japanese" & data$level=="word" & data$subalgorithm=="BTP_abs"),]$token_fscore)
# btpajm<-mean(data[which(data$language=="Japanese" & data$level=="morphemes" & data$subalgorithm=="BTP_abs"),]$token_fscore)
# 
# ftpacw<-mean(data[which(data$language=="Chintang" & data$level=="word" & data$subalgorithm=="FTP_abs"),]$token_fscore)
# ftpacm<-mean(data[which(data$language=="Chintang" & data$level=="morphemes" & data$subalgorithm=="FTP_abs"),]$token_fscore)
# ftpajw<-mean(data[which(data$language=="Japanese" & data$level=="word" & data$subalgorithm=="FTP_abs"),]$token_fscore)
# ftpajm<-mean(data[which(data$language=="Japanese" & data$level=="morphemes" & data$subalgorithm=="FTP_abs"),]$token_fscore)
# 
# btprcw<-mean(data[which(data$language=="Chintang" & data$level=="word" & data$subalgorithm=="BTP_rel"),]$token_fscore)
# btprcm<-mean(data[which(data$language=="Chintang" & data$level=="morphemes" & data$subalgorithm=="BTP_rel"),]$token_fscore)
# btprjw<-mean(data[which(data$language=="Japanese" & data$level=="word" & data$subalgorithm=="BTP_rel"),]$token_fscore)
# btprjm<-mean(data[which(data$language=="Japanese" & data$level=="morphemes" & data$subalgorithm=="BTP_rel"),]$token_fscore)
# 
# ftprcw<-mean(data[which(data$language=="Chintang" & data$level=="word" & data$subalgorithm=="FTP_rel"),]$token_fscore)
# ftprcm<-mean(data[which(data$language=="Chintang" & data$level=="morphemes" & data$subalgorithm=="FTP_rel"),]$token_fscore)
# ftprjw<-mean(data[which(data$language=="Japanese" & data$level=="word" & data$subalgorithm=="FTP_rel"),]$token_fscore)
# ftprjm<-mean(data[which(data$language=="Japanese" & data$level=="morphemes" & data$subalgorithm=="FTP_rel"),]$token_fscore)
# 
# dibscw<-mean(data[which(data$language=="Chintang" & data$level=="word" & data$subalgorithm=="DiBS"),]$token_fscore)
# dibscm<-mean(data[which(data$language=="Chintang" & data$level=="morphemes" & data$subalgorithm=="DiBS"),]$token_fscore)
# dibsjw<-mean(data[which(data$language=="Japanese" & data$level=="word" & data$subalgorithm=="DiBS"),]$token_fscore)
# dibsjm<-mean(data[which(data$language=="Japanese" & data$level=="morphemes" & data$subalgorithm=="DiBS"),]$token_fscore)
# 
# base05cw<-mean(data[which(data$language=="Chintang" & data$level=="word" & data$subalgorithm=="Baseline0.5"),]$token_fscore)
# base05cm<-mean(data[which(data$language=="Chintang" & data$level=="morphemes" & data$subalgorithm=="Baseline0.5"),]$token_fscore)
# base05jw<-mean(data[which(data$language=="Japanese" & data$level=="word" & data$subalgorithm=="Baseline0.5"),]$token_fscore)
# base05jm<-mean(data[which(data$language=="Japanese" & data$level=="morphemes" & data$subalgorithm=="Baseline0.5"),]$token_fscore)
# 
# base0cw<-mean(data[which(data$language=="Chintang" & data$level=="word" & data$subalgorithm=="Baseline0"),]$token_fscore)
# base0cm<-mean(data[which(data$language=="Chintang" & data$level=="morphemes" & data$subalgorithm=="Baseline0"),]$token_fscore)
# base0jw<-mean(data[which(data$language=="Japanese" & data$level=="word" & data$subalgorithm=="Baseline0"),]$token_fscore)
# base0jm<-mean(data[which(data$language=="Japanese" & data$level=="morphemes" & data$subalgorithm=="Baseline0"),]$token_fscore)
# 
# base1cw<-mean(data[which(data$language=="Chintang" & data$level=="word" & data$subalgorithm=="Baseline1"),]$token_fscore)
# base1cm<-mean(data[which(data$language=="Chintang" & data$level=="morphemes" & data$subalgorithm=="Baseline1"),]$token_fscore)
# base1jw<-mean(data[which(data$language=="Japanese" & data$level=="word" & data$subalgorithm=="Baseline1"),]$token_fscore)
# base1jm<-mean(data[which(data$language=="Japanese" & data$level=="morphemes" & data$subalgorithm=="Baseline1"),]$token_fscore)
# 
# 
# matrix<-matrix( c("AG", round(agcw,3), round(agcm,3), round(agjw,3), round(agjm,3), "BTPa", round(btpacw,3), round(btpacm,3), round(btpajw,3), round(btpajm,3), "FTPa", round(ftpacw,3), round(ftpacm,3), round(ftpajw,3), round(ftpajm,3), "BTPr", round(btprcw,3), round(btprcm,3), round(btprjw,3), round(btprjm,3), "FTPr", round(ftprcw,3), round(ftprcm,3), round(ftprjw,3), round(ftprjm,3), "DiBS", round(dibscw,3), round(dibscm,3), round(dibsjw,3), round(dibsjm,3), "Baseline0.5",round(base05cw,3), round(base05cm,3), round(base05jw,3), round(base05jm,3), "Baseline0",round(base0cw,3), round(base0cm,3), round(base0jw,3), round(base0jm,3),"Baseline1",round(base1cw,3), round(base1cm,3), round(base1jw,3), round(base1jm,3) ), nrow=9,   ncol=5, byrow=TRUE)

write.table(mytab, paste0(outfolder,"meanscores.txt"), row.names=F, sep="\t&\t", quote = F )

```

Mean F-scores are shown for languages, algorithms, and levels.
The baselines capture random segmentations of the syllabified corpus. Baseline 0.5 adds word boundaries with a probability ($p$)  of $0.5$ at each syllable boundary. Baseline 0 uses $p=0$ to generate an utterance baseline, considering each utterance as a word. Baseline 1 has $p=1$ cuts at all syllable boundaries, thus treating every syllable as a word.

```{r tab:anovascores, echo=FALSE}
library(car)
fitanova<-Anova(mainLM, type=3)
table <- as.data.frame(fitanova)
table
table[,-1] <-round(table[,-1],3)
write.table(table,paste0(outfolder,"anovascores.txt"), sep="\t&\t",quote = F)
```

Analysis of variance (anova type III) results for all factors (language, algorithm and level) and their interactions on a linear regression where token F-scores are the dependent variable.

```{r tab:anovascoresalgo, echo=FALSE}
getstars <- function(df) {
  ps=Anova(lm(token_fscore ~ language * level, df), type=3)[c(2:4),4] #get p-values from Anova
  stars=ifelse(ps < .001,"***",ifelse(ps < .01,"**",ifelse(ps < .05,"*","")))
  stars
} 

mytab=NULL
for(thisalgo in c("AG", "BTPa", "FTPa", "BTPr", "FTPr", "DiBS")){
  mytab=cbind(mytab,getstars(data[data$subalgorithm==thisalgo,]))
  colnames(mytab)[dim(mytab)[2]]<-thisalgo
} 

rownames(mytab)<-c("language", "level", "language:level")
mytab
write.table(mytab, paste0(outfolder,"anovaalgorithms.txt"), sep="\t&\t", quote = F )

```

Analysis of variance (anova) significance results for linear regressions where token F-scores are predicted by language and level within each algorithm type.



```{r entropy-ana}
entropy_LM=summary(lm(token_fscore~corpus_entropy*language*subalgorithm + (1 / file), subset=c(level=="words"), parts ))
entropy_LM
```


We investigated whether these effects may be due to potential confounds, in particular the possibility that one of the languages is intrinsically more ambiguous to segment. To this effect, we estimated the segmentation entropy of the corpora using the Normalized Segmentation Entropy formula of \citet{fourtassi2013whyisenglishsoeasytosegment}, and focused on the word level, as they had. In their study, English was less ambiguous than Japanese, with entropies of .0021 and .0156 respectively. The segmentation entropy of the Japanese ACQDIV corpus was `r j_w$corpus_entropy`, thus close to their Japanese results. Surprisingly,  for Chintang it was `r  ch_w$corpus_entropy` - closer to English than to Japanese. As obvious from this description, in the present sample morphological complexity and ambiguity (indexed by entropy) were not correlated. Moreover, in a regression analysis, where the other two factors (language and algorithm) were included as well, entropy failed to explain any variance, with a non-significant coefficient estimate of `r entropy_LM$coefficients[2,1]`( `r entropy_LM$coefficients[2,2] `); notice that the sign is nonetheless in line with \citet{fourtassi2013whyisenglishsoeasytosegment}'s predictions that lower entropy will lead to higher scores.


```{r wl-ana}
word_length_LM=summary(lm(token_fscore~phonesPerWord*language*subalgorithm + (1 / file), subset=c(level=="words"), parts ))
word_length_LM
```

We also inquired whether sheer word length, previously evoked by \citet{saksida2017co}, may explain away the language effects found here. As a reminder, Chintang has an average word length of `r round(ch_w$phones_tokens/ch_w$words_tokens,3) ` phones and `r round(ch_w$syllables_tokens/ch_w$words_tokens,3)` syllables; whereas for Japanese these are `r round(j_w$phones_tokens/ch_w$words_tokens,3) ` phones and `r round(j_w$syllables_tokens/ch_w$words_tokens,3)` respectively. A regression predicting token f-score on the word level from average word length (in phonemes), among the other two factors (language and algorithm), across the corpora led to a non-significant coefficient estimate of `r word_length_LM$coefficients[2,1]`( `r word_length_LM$coefficients[2,2] `). 

```{r sentl-ana}
sent_length_LM=summary(lm(token_fscore~syllablesPerUtt*language*subalgorithm + (1 / file), subset=c(level=="words"), parts ))
sent_length_LM
```
The number of syllables per utterance, which is `r round(ch_w$syllables_tokens/ch_w$corpus_nutts,3) `  for Chintang and `r round(j_w$syllables_tokens/j_w$corpus_nutts,3) ` for Japanese, is also non-significant in a similar regression `r sent_length_LM$coefficients[2,1]`( `r sent_length_LM$coefficients[2,2] `). 

```{r rep-ana}

wtt_LM=summary(lm(token_fscore~wtokens_wtypes_ratio*language*subalgorithm + (1 / file), subset=c(level=="words"), parts ))
wtt_LM

hap_LM=summary(lm(token_fscore~words_hapax_ratio*language*subalgorithm + (1 / file), subset=c(level=="words"), parts ))
hap_LM
```

Additionally, we mentioned above that a higher proportion of hapaxes and few repetitions of each word token (since each lexeme can have different surface forms) which might affect performance in lexical algorithms, thus we searched whether token/type ratios could account for (some of) our results. We found a non-significant coefficient of  `r wtt_LM$coefficients[2,1]`( `r wtt_LM$coefficients[2,2] `). The hapax/ type ratio coefficient is also non-significant,  `r hap_LM$coefficients[2,1]`( `r hap_LM$coefficients[2,2] `). 

```{r cor-stats-explo}
getR2<-function(x) round(summary(x)$adj.r.squared,3)

#the R2 for the main lm is...
 getR2(lm(token_fscore~language*subalgorithm + (1 / file), parts ,subset=c(level=="words")))

R2entropy <- getR2(lm(token_fscore~corpus_entropy + (1 / file), parts ,subset=c(level=="words")))
R2syllutter <- getR2(lm(token_fscore~syllablesPerUtt + (1 / file), parts,subset=c(level=="words")))
R2phoneword <- getR2(lm(token_fscore~phonesPerWord + (1 / file), parts,subset=c(level=="words")))
R2tokentype <- getR2(lm(token_fscore~wtokens_wtypes_ratio + (1 / file), parts,subset=c(level=="words"))) 
R2hapaxtype <- getR2(lm(token_fscore~words_hapax_ratio + (1 / file), parts ,subset=c(level=="words")))

R2entropynorm <- getR2(lm(token_fscore~corpus_entropy*language*subalgorithm + (1 / file), parts ,subset=c(level=="words")))
R2phonewordnorm <- getR2(lm(token_fscore~phonesPerWord*language*subalgorithm + (1 / file), parts ,subset=c(level=="words")))
R2syllutternorm <- getR2(lm(token_fscore~syllablesPerUtt*language*subalgorithm + (1 / file), parts ,subset=c(level=="words")))
R2tokentypenorm <- getR2(lm(token_fscore~wtokens_wtypes_ratio*language*subalgorithm + (1 / file), parts ,subset=c(level=="words")))
R2hapaxtypenorm <- getR2(lm(token_fscore~words_hapax_ratio*language*subalgorithm + (1 / file), parts ,subset=c(level=="words")))


array1<-c(R2entropy,R2phoneword,R2syllutter,R2tokentype,R2hapaxtype)
array2<-c(R2entropynorm,R2phonewordnorm,R2syllutternorm,R2tokentypenorm,R2hapaxtypenorm)
fit_statistics<-cbind(array1,array2)
colnames(fit_statistics)<-c("$R^2$ Factor","$R^2$ Factor*Lang*Algo")
row.names(fit_statistics)<-c("entropy (NSA)","phones/word","syll/utt","w tok/typ","prop. hapax")
fit_statistics
write.table(fit_statistics, paste0(outfolder,"tab_fit_statistics.txt"), sep="\t&\t", quote = F )
```

Percentage of variance explained ($R^2$) predicting F-scores (word level only) from 1. only the factor given in the first column; 2. language and algorithm (but not the potentially confounded factor); 3. language, algorithm, and the factor.


## Appendix

### check -- the necessary results do not exist yet.

```{r tab:meanscores_full, eval=RECALC}
scores_full<-cbind(wholes[wholes$language =='Chintang' & wholes$level =="word","token_fscore"], 
                       wholes[wholes$language =='Japanese' & wholes$level =="word","token_fscore"], 
                       wholes[wholes$language =='Chintang' & wholes$level =="morphemes","token_fscore"],
                       wholes[wholes$language =='Japanese' & wholes$level =="morphemes","token_fscore"])
rownames(scores_full)<-levels(wholes$subalgorithm)
colnames(scores_full)<-c("word Chintang", "word Japanese", "morpheme Chintang", "morpheme Japanese")
scores_full
write.table(scores_full, paste0(outfolder,"scores_full.txt"),  sep="\t&\t", quote = F )

```

Token F-scores are shown for languages, algorithms, ran on the whole corpora.


```{r tab:table-regres-complete}
round(summary(mainLM)$coefficients[,c(1:2)],4)->temp
ps=summary(mainLM)$coefficients[,4]
stars=ifelse(ps < .001,"***",ifelse(ps < .01,"**",ifelse(ps < .05,"*","")))
mytab=cbind(paste0(temp[,1]," (", temp[,2],")"),stars)
colnames(mytab)<-c("Estimate", "Pr($>|t|$)") 
mytab
write.table(mytab,paste0(outfolder,"tab_detailed_fit.txt"),sep="\t&\t",eol="\n",quote = F)

```

Regression results for factors and interactions.

### TO FIX, looks empty

```{r fig:prerew}
library(ggplot2)
worddata <- parts[parts$level =='word',]
p <- ggplot(worddata, aes(token_precision, y=token_recall)) + ggtitle( 'Word Level, precision and recall')  + geom_point(aes(colour=language, shape=subalgorithm), size = 3) 
p<-p+ theme(panel.background = element_rect(fill = 'white', colour = 'black'))
p + scale_color_hue(l=40, c=35)
p<-p+ scale_x_continuous(limits = c(0.2, 0.7))
p<-p+ scale_y_continuous(limits = c(0.2, 0.7))
print(p)
```

Precision and Recall for all algorithms and languages on the word representational level.

### works OK

```{r fig:prerem}
morphdata <- parts[ data$level =='morphemes',]
p <- ggplot(morphdata, aes(token_precision, y=token_recall)) + ggtitle( 'Morpheme Level, precision and recall')  + geom_point(aes(colour=language, shape=subalgorithm), size = 3) 
p<-p+ theme(panel.background = element_rect(fill = 'white', colour = 'black'))
p + scale_color_hue(l=40, c=35)
p<-p+ scale_x_continuous(limits = c(0.2, 0.7))
p<-p+ scale_y_continuous(limits = c(0.2, 0.7))
print(p)
```

Precision and Recall for all algorithms and languages on the morpheme representational level.

### check -- the necessary results do not exist yet.

```{r tab:noforeign, echo=FALSE, eval=RECALC}
data<-read.csv(paste0(outfolder,"/res-stat-noforeign.txt"))
subset(data,!is.na(corpus))->data 

chintang_only<-cbind(data[data$language =='Chintang' & data$level =="morphemes","token_fscore"], 
                      data[data$language =='Chintang' & data$level =="word","token_fscore"])
colnames(chintang_only)=c("morphemes", "words")
write.table(chintang_only, paste0(outfolder,"chintang_only.txt"),  sep="\t&\t", quote = F )

```

Token F-scores are shown for Chintang without Nepali.
