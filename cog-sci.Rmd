---
title: "Supplementary materials to wordseg-xlling"
author: "alejandrina cristia"
date: "4/27/2018"
output: html_document
---

```{r setup, include=FALSE}

RECALC=FALSE #turn TRUE if you need to re-run the analyses from scratch (i.e. results or stats have changed)

knitr::opts_chunk$set(echo = TRUE)
args<-commandArgs(trailingOnly=TRUE)
resfile=args[1]
statfile=args[2]
folder=args[3]
resfile<-"results.txt"
statfile="stats.txt"
folder="/scratch1/users/acristia/acqdiv/" #name of the root where files where stored
outfolder="." #location you want all figures and temp data to be stored in

```

## Data pre-processing



```{r intro, eval=RECALC}

#COMPILE RESULTS DATA
data<-read.csv(resfile)
# colnames(data)<-c("folder" ,
# "token_precision" ,"token_recall", "token_fscore" ,
# "type_precision", "type_recall", "type_fscore" ,
# "boundary_precision", "boundary_recall", "boundary_fscore" ,
# "boundary_NE_precision" ,"boundary_NE_recall" ,"boundary_NE_fscore")
for(thiscol in c("boundary_NE_precision" ,"boundary_NE_recall" ,"boundary_NE_fscore")) data[,thiscol]=as.numeric(as.character(data[,thiscol]))
summary(data)
data$fscores=data$token_fscore
#clean up and extract info from files named like /scratch1/users/acristia/acqdiv//Chintang/morphemes/full-tags.txt_eval.ag.txt
# and like Japanese/words/full_part9/tags.txt_eval.ftp_rel.txt
data$folder=gsub("//","/",data$folder)
data$folder=gsub(folder,"",data$folder)
data$language=gsub("/.*","",data$folder)
data$level=gsub("/.*","",gsub(".*[ge]/","",data$folder))
data$subalgorithm=gsub(".txt","",gsub(".*eval.","",data$folder))
data$file=gsub("tags.txt.*","",data$folder)
  
# divergence in notation between full and part
data$corpus=NA
data$corpus[grep("_part",data$folder,invert=T)]=gsub("-tags","",gsub("\\..*","",gsub(".*/","",data$folder[grep("_part",data$folder,invert=T)])))
data$subcorpus=NA
data$subcorpus[grep("_part",data$folder)]=gsub(".*/","",gsub("/tags.*","",data$folder[grep("_part",data$folder)]))

summary(data)

#COMPILE STATS DATA
stat<-read.csv(statfile)
stat$folder=gsub("//","/",stat$folder)
stat$folder=gsub(folder,"",stat$folder)
stat$file=gsub("stats.txt.*","",stat$folder)

#combine and save
merge(data,stat,by="file")->data
write.table(data,paste0(outfolder,"/res-stat.txt"),row.names=F,quote=T,sep=",")

```

##Some corpus statistics exist at the Section 2.1 - maybe we should mention them as well. This paragraph:
All child-directed and child-overheard speech had been carefully transcribed in a transparent orthography (as well as the child's own speech, which was not analyzed here). After processing, the Chintang corpus contained  296,939 utterances, with an average of 2.7 words, 5.4 syllables, and 11.4 phones per utterance; and the Japanese one 264,945 utterances (3 words, 5.7 syllables, 11.2 phones per utterance). Both corpora are approximately ten times larger than those frequently used for modeling studies \citep{saksida2017co,phillips2014bayesian}. All utterances where morpheme annotation was incomplete were removed, so the Japanese morpheme corpus after processing contained 85267 utterances with 2 morphemes, 3.2 syllables, 6.3 phones per utterance and the Chintang morpheme corpus contained 280319 utterances with 4.5 morphemes, 5.5 syllables, 11.4 phones per utterance.

## General results section




```{r res-par1, echo=FALSE}
read.csv(paste0(outfolder,"/res-stat.txt"))->data
fit <- lm(fscores ~ language * level * subalgorithm + (1 / file), data)
 f <- summary(fit)$fstatistic
 p <- pf(f[1],f[2],f[3],lower.tail=F)
 coefs <- summary(fit)$coefficients
```

The code above allowed us to fill in this paragraph:
Throughout the analyses, we focused on token F-scores separately for each combination of language, level, and type of algorithm. A regression predicting F-scores from language, level, algorithm and their interactions accounted for most of the variance in the data, $R^2=$.`r round(summary(fit)$adj.r.squared,2)` ($F($`r summary(fit)$df[1]-1`, `r summary(fit)$df[2]`) $=$ `r summary(fit)$fstatistic[1]`, $p < $ `r round(p,3)`). As predicted, the coefficient estimating the language effect between Chintang and Japanese is positive (`r round(coefs["languageJapanese","Estimate"],2))`), suggesting higher scores for the latter. Interestingly, this effect is smaller than that of level (`r round(coefs["levelwords","Estimate"],2)`, indicating evaluation on words lead to lower scores than that on morphemes). The presence of all 2- and 3-way interactions, however, discourages a simple reading of these main effects. As clear in Figure \ref{fig:res} and Table \ref{tab:meanscores}, there were strong interactions between all three factors. Information on the regression results can be found on Tables \ref{tab:anovascores} and \ref{tab:anovascoresalgo}. 

```{r fig:res, echo=FALSE}
pdf(paste0(outfolder,"fig:res.pdf"))
palette(c("black", "red", "green3", "blue", "orange", "gray", "violet", "brown","green1"))
newdata <- data
newdata$subalgorithm = factor(newdata$subalgorithm, c("AGu", "BTP_abs", "FTP_abs", "BTP_rel", "FTP_rel", "DiBS","base1","base0.5","base0"))
newdata$xval= ifelse(newdata$level=="words",0,2) + ifelse(newdata$language=="Chintang",1,2) + (as.numeric(newdata$subalgorithm)/10 - mean(as.numeric(newdata$subalgorithm)/10))
par(mar=c(5.1, 4.1, 4.1, 8.1), xpd=TRUE)
#part<-split(newdata$token_fscore, 24)
#plot(newdata[1:10]$token_fscore~newdata$xval,cex=3, type="n")
plot(newdata$token_fscore~newdata$xval,col=as.numeric(as.factor(newdata$subalgorithm), palette), pch="x",xaxt='n',xlab="", ylab='Token F-scores', ylim=c(0.1, 0.8), cex.lab = 1.3, cex.axis = 1, main="F-scores for language, algorithm and level", cex.main=1.3)
axis(1,at=1:4,labels=c("Chintang","Japanese","Chintang","Japanese"), cex.axis = 1.3)
axis(1,at=c(1.5,3.5),labels=c("Words","Morphemes"),line=2, cex.axis = 1.3)
legend('topright', inset=c(-0.25,0), legend=c('AGu', 'BTPa', 'FTPa', 'BTPr', 'FTPr', 'DiBS', "Baseline1", "Baseline0.5", "Baseline0"), pch=c("x","x","x","x","x","x", "x", "x", "x"), col=c('black','red', 'green3', 'blue','orange', 'grey', "violet", "brown", "green1"), cex=1, pt.cex=1)

dev.off()
```
Token F-scores across language and representational level. Models are marked by color. Datapoints spread vertically are the results for the ten subsets of each corpus.

```{r tab:meanscores, echo=FALSE}
plot
```

Mean F-scores are shown for languages, algorithms, and levels.
The baselines capture random segmentations of the syllabified corpus. Baseline 0.5 adds word boundaries with a probability ($p$)  of $0.5$ at each syllable boundary. Baseline 0 uses $p=0$ to generate an utterance baseline, considering each utterance as a word. Baseline 1 has $p=1$ cuts at all syllable boundaries, thus treating every syllable as a word.

```{r tab:anovascores, echo=FALSE}
plot(pressure)
```
Analysis of variance (anova type III) results for all factors (language, algorithm and level) and their interactions on a linear regression where token F-scores are the dependent variable.
```{r tab:anovascoresalgo, echo=FALSE}
plot(pressure)
```
Analysis of variance (anova) significance results for linear regressions where token F-scores are predicted by language and level within each algorithm type.

```{r respar2, echo=FALSE}
plot(pressure)
```
We investigated whether these effects may be due to potential confounds, in particular the possibility that one of the languages is intrinsically more ambiguous to segment. To this effect, we estimated the segmentation entropy of the corpora using the Normalized Segmentation Entropy formula of \citet{fourtassi2013whyisenglishsoeasytosegment}, and focused on the word level, as they had. In their study, English was less ambiguous than Japanese, with entropies of $.0021$ and $.0156$ respectively. The segmentation entropy of the Japanese ACQDIV corpus was $.014$, thus close to their Japanese results. Surprisingly,  for Chintang it was $.007$ - closer to English than to Japanese. As obvious from this description, in the present sample morphological complexity and ambiguity (indexed by entropy) were not correlated. Moreover, in a regression analysis, where the other two factors (language and algorithm) were included as well, entropy failed to explain any variance, with a non-significant coefficient estimate of $ -10.70 (16.04)$; notice that the sign is nonetheless in line with \citet{fourtassi2013whyisenglishsoeasytosegment}'s predictions that lower entropy will lead to higher scores.

```{r respar3, echo=FALSE}
plot(pressure)
```

We also inquired whether sheer word length, previously evoked by \citet{saksida2017co}, may explain away the language effects found here. As a reminder, Chintang has an average word length of 4.23 phones and 2.02 syllables; whereas for Japanese these are 3.74 and 1.91 respectively. A regression predicting Token F-score on the word level from average word length (in phonemes), among the other two factors (language and algorithm), across the corpora led to a non-significant coefficient estimate of $.1281 (.1688)$. The number of syllables per utterance, which is 5.37 for Chintang and 5.51 for Japanese, is also non-significant for the same regression $-.0035 (.025)$. Additionally, we mentioned above that a higher proportion of hapaxes and few repetitions of each word token (since each lexeme can have different surface forms) which might affect performance in lexical algorithms, thus we searched whether token/type ratios could account for (some of) our results. We found a non-significant coefficient of  $-.0285 (.0310)$. The hapax/ type ratio coefficient is also non-significant, $1.5657 (1.0063)$. 

```{r respar4, echo=FALSE}
plot(pressure)
```
Percentage of variance explained ($R^2$) predicting F-scores (word level only) from 1. only the factor given in the first column; 2. language and algorithm (but not the potentially confounded factor); 3. language, algorithm, and the factor.


## Appendix
```{r tab:meanscores, echo=FALSE}
plot(pressure)
```
Token F-scores are shown for languages, algorithms, ran on the whole corpora.

```{r tab:table-regres-complete, echo=FALSE}
plot(pressure)
```

Regression results for factors and interactions.

```{r fig:prerew, echo=FALSE}
plot(pressure)
```

Precision and Recall for all algorithms and languages on the word representational level.

```{r fig:prerem, echo=FALSE}
plot(pressure)
```

Precision and Recall for all algorithms and languages on the word representational level.

```{r tab:noforeign, echo=FALSE}
plot(pressure)
```
Token F-scores are shown for Chintang without Nepali.
